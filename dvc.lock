schema: '2.0'
stages:
  data_preprocessing:
    cmd: python scripts/data_prep_script.py
    deps:
    - path: configs/data_prep_config.yaml
      hash: md5
      md5: 83db1f223c396ae71aadac2f8b924373
      size: 1491
    - path: data/raw/bitext-retail-ecommerce-llm-chatbot-training-dataset.csv
      hash: md5
      md5: 6dd684fa00ea6f065b0b7c9375e01675
      size: 42624013
    - path: scripts/data_prep_script.py
      hash: md5
      md5: 9eff00ca0dd5ba07ccb428a0d0951165
      size: 3607
    - path: src/data_prep.py
      hash: md5
      md5: fcbb3731279b1b77286ada28737710f4
      size: 8729
    params:
      params.yaml:
        sampling.sample_description: small_dataset_for_testing_pipeline
        sampling.sample_size: 10
    outs:
    - path: data/processed/latest_dataset_ref.json
      hash: md5
      md5: ff983707ad1248a4134b5316e3737390
      size: 225
  fine_tuning:
    cmd: python scripts/fine_tuning_script.py  --input-ref data/processed/latest_dataset_ref.json  --output-dir
      results/  --config params.yaml  --mlflow-uri https://dagshub.com/ShenghaoisYummy/E-commerce-Chatbot.mlflow
    deps:
    - path: data/processed/latest_dataset_ref.json
      hash: md5
      md5: ff983707ad1248a4134b5316e3737390
      size: 225
    - path: params.yaml
      hash: md5
      md5: 1e2ac9a58a23db46225a66184f2df339
      size: 1246
    - path: scripts/fine_tuning_script.py
      hash: md5
      md5: f5ef9b4156dd509ece7b3c2299e1815c
      size: 7629
    - path: src/fine_tuning.py
      hash: md5
      md5: ae74d3f95e360ba34734fbd57ccd323d
      size: 7364
    params:
      params.yaml:
        data:
          dataset_path: 
            data/processed/processed_dataset_10rows_small_dataset_for_testing_pipeline_20250502_132555.csv
          test_size: 0.1
          seed: 42
          instruction_column: instruction
          response_column: response
        lora:
          r: 16
          alpha: 32
          dropout: 0.1
          target_modules:
          - q_proj
          - v_proj
          - k_proj
          - o_proj
          - gate_proj
          - up_proj
          - down_proj
        model:
          base_model: EleutherAI/gpt-neo-125m
          load_in_8bit: false
        training:
          epochs: 1
          batch_size: 1
          gradient_accumulation_steps: 1
          learning_rate: 0.0002
          weight_decay: 0.01
          warmup_steps: 100
          max_length: 256
          fp16: false
          eval_strategy: epoch
          save_strategy: epoch
          save_total_limit: 2
          load_best_model_at_end: true
    outs:
    - path: results/fine_tuned_model_location.json
      hash: md5
      md5: 65525b84654e1de98ec514b7e2f5d376
      size: 255
